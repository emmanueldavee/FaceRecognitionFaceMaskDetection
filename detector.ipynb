{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_predict_mask(frame, faceNet, maskNet):\n",
    "    # Get height and width of the frame\n",
    "    (h, w) = frame.shape[:2]\n",
    "    \n",
    "    # Preprocess image by getting the blob before face detection using caffe model\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.3, (224, 224), (104.0, 177.0, 123.0), False, False)\n",
    "    \n",
    "    # Pass blob into the caffe model\n",
    "    faceNet.setInput(blob)\n",
    "    \n",
    "    # Get face detections\n",
    "    detections = faceNet.forward()\n",
    "    \n",
    "    faces = []\n",
    "    locs = []\n",
    "    preds = []\n",
    "    \n",
    "    # Iterate over all faces detected by the caffe model\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        \n",
    "        # Get the confidence value\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        \n",
    "        # if confidence passes threshold, it can be processed further\n",
    "        if confidence > 0.5:\n",
    "            \n",
    "            # Get the bounding box of the face in the frame\n",
    "            box = detections[0,0,i,3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            \n",
    "            # Ensure that bounding box still lies inside the frame\n",
    "            (startX, startY) = (max(0, startX), max(0, startY))\n",
    "            (endX, endY) = (min(w-1, endX), min(h-1, endY))\n",
    "            \n",
    "            # Crop image to only the face\n",
    "            face = frame[startY:endY, startX:endX]\n",
    "            \n",
    "            # Convert face into RGB format before fitting it into the model\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Resize image to the standard size\n",
    "            face = cv2.resize(face, (224,224))\n",
    "            \n",
    "            # convert image into an array\n",
    "            face = img_to_array(face)\n",
    "            \n",
    "            # Preprocess image to meet the format of MobileNet\n",
    "            face = preprocess_input(face)\n",
    "            \n",
    "            # Append filtered face into a list\n",
    "            faces.append(face)\n",
    "            \n",
    "            # Append the bounding box into a list\n",
    "            locs.append((startX, startY, endX, endY))\n",
    "            \n",
    "    if len(faces) > 0:\n",
    "        # Convert face into a numpy array\n",
    "        faces = np.array(faces, dtype=\"float32\")\n",
    "        \n",
    "        # Predict the face using the loaded Mask Detector\n",
    "        preds = maskNet.predict(faces, batch_size=32)\n",
    "        \n",
    "    return (locs, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_recognize(frame, model, rect):\n",
    "    name = \"\"\n",
    "    conf = -1\n",
    "    \n",
    "    # Get the bounding box obtained from the mask detector preprocessing\n",
    "    (startX, startY, endX, endY) = rect\n",
    "    \n",
    "    # Crop frame to only the bounding box\n",
    "    gray_cropped = frame[startY:endY, startX:endX]\n",
    "    \n",
    "    # Convert cropped face into grayscale\n",
    "    gray_cropped = cv2.cvtColor(gray_cropped, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    try:\n",
    "        # Use the trained LBPH Face Recognizer to identify the subject in the frame\n",
    "        predicted, conf = model.predict(gray_cropped)\n",
    "        \n",
    "        # Transform prediction into its corresponding name\n",
    "        if(predicted == 0):\n",
    "            name =  \"Chaeyoung\"\n",
    "        elif(predicted == 1):\n",
    "            name = \"Dahyun\"\n",
    "        elif(predicted == 2):\n",
    "            name = \"Dave\"\n",
    "        elif(predicted == 3):\n",
    "            name = \"Jeongyeon\"\n",
    "        elif(predicted == 4):\n",
    "            name = \"Jihyo\"\n",
    "        elif(predicted == 5):\n",
    "            name = \"Mina\"\n",
    "        elif(predicted == 6):\n",
    "            name = \"Momo\"\n",
    "        elif(predicted == 7):\n",
    "            name = \"Nayeon\"\n",
    "        elif(predicted == 8):\n",
    "            name = \"Sana\"\n",
    "        elif(predicted == 9):\n",
    "            name = \"Tzuyu\"\n",
    "\n",
    "    except:\n",
    "        print('No Faces found')\n",
    "        pass\n",
    "            \n",
    "    return name, conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Caffe Model for Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototxtPath = r\"face_detector\\deploy.prototxt.txt\"\n",
    "weightsPath = r\"face_detector\\res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "\n",
    "faceNet = cv2.dnn.readNetFromCaffe(prototxtPath, weightsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trained Mask Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskNet = load_model(\"mask_detector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trained LBPH Face Recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbph = cv2.face.LBPHFaceRecognizer_create()\n",
    "model = lbph.read('TrainedLBPHFaceRecognizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dave: 32.6\n",
      "Dave: 30.7\n",
      "Dave: 32.4\n",
      "Dave: 30.7\n",
      "Dave: 30.9\n",
      "Dave: 31.4\n",
      "Dave: 31.2\n",
      "Dave: 30.7\n",
      "Dave: 30.8\n",
      "Dave: 31.5\n",
      "Dave: 32.3\n",
      "Dave: 33.3\n",
      "Dave: 31.2\n",
      "Dave: 31.5\n",
      "Dave: 33.7\n",
      "Dave: 33.2\n",
      "Dave: 31.4\n",
      "Dave: 31.7\n",
      "Dave: 31.2\n",
      "Dave: 31.7\n",
      "Dave: 31.4\n",
      "Dave: 30.2\n",
      "Dave: 34.1\n",
      "Dave: 31.7\n",
      "Dave: 31.8\n",
      "Dave: 31.0\n",
      "Dave: 31.3\n",
      "Dave: 31.3\n",
      "Dave: 31.8\n",
      "Dave: 33.6\n",
      "Dave: 33.9\n",
      "Dave: 32.4\n",
      "Dave: 33.4\n",
      "Dave: 33.1\n",
      "Dave: 30.1\n",
      "Dave: 30.1\n",
      "Dave: 30.2\n",
      "Dave: 30.0\n",
      "Dave: 30.5\n",
      "Dave: 31.1\n",
      "Dave: 30.0\n",
      "Dave: 31.2\n",
      "Dave: 32.4\n",
      "Dave: 32.7\n",
      "Dave: 32.8\n",
      "Dave: 31.5\n",
      "Dave: 31.2\n",
      "Dave: 33.1\n",
      "Dave: 32.5\n",
      "Dave: 32.5\n",
      "Dave: 32.1\n",
      "Dave: 31.2\n",
      "Dave: 32.5\n",
      "Dave: 33.8\n",
      "Dave: 32.1\n",
      "Dave: 32.0\n",
      "Dave: 31.9\n",
      "Dave: 30.0\n",
      "Dave: 30.8\n",
      "Dave: 33.9\n",
      "Dave: 35.3\n",
      "Dave: 32.4\n",
      "Dave: 30.6\n",
      "Dave: 30.5\n",
      "Dave: 31.2\n",
      "Dave: 30.0\n",
      "Dave: 32.6\n",
      "Dave: 32.0\n",
      "Dave: 31.7\n",
      "Dave: 31.6\n",
      "Dave: 32.5\n",
      "Dave: 30.6\n",
      "Dave: 31.4\n",
      "Dave: 31.2\n",
      "Dave: 31.3\n",
      "Dave: 30.7\n",
      "Dave: 31.5\n",
      "Dave: 31.7\n",
      "Dave: 30.5\n",
      "Dave: 31.0\n",
      "Dave: 31.8\n",
      "Dave: 32.0\n",
      "Dave: 31.0\n",
      "Dave: 30.0\n",
      "Dave: 30.9\n",
      "Dave: 31.8\n",
      "Dave: 32.2\n",
      "Dave: 31.5\n",
      "Dave: 31.9\n",
      "Dave: 31.5\n",
      "Dave: 32.6\n",
      "Dave: 32.1\n",
      "Dave: 31.4\n",
      "Dave: 32.0\n",
      "Dave: 32.8\n",
      "Dave: 33.5\n",
      "Dave: 32.3\n",
      "Dave: 33.5\n",
      "Dave: 34.1\n",
      "Dave: 33.8\n",
      "Dave: 33.7\n",
      "Dave: 34.2\n",
      "Dave: 35.2\n",
      "Dave: 31.2\n",
      "Dave: 32.6\n",
      "Dave: 34.2\n",
      "Dave: 31.7\n",
      "Dave: 30.0\n",
      "Dave: 35.9\n",
      "Dave: 34.7\n",
      "Dave: 32.8\n",
      "Dave: 36.0\n",
      "Dave: 36.8\n",
      "Dave: 37.6\n",
      "Dave: 38.3\n",
      "Dave: 37.7\n",
      "Dave: 35.5\n",
      "Dave: 36.2\n",
      "Dave: 35.2\n",
      "Dave: 36.2\n",
      "Dave: 35.0\n",
      "Dave: 32.4\n",
      "Dave: 31.3\n",
      "Dave: 32.2\n",
      "Dave: 32.0\n",
      "Dave: 30.7\n",
      "Dave: 30.7\n",
      "Dave: 31.5\n",
      "Dave: 31.3\n",
      "Dave: 31.2\n",
      "Dave: 30.6\n",
      "Dave: 31.8\n",
      "Dave: 31.0\n",
      "Dave: 33.7\n",
      "Dave: 34.4\n",
      "Dave: 33.4\n",
      "Dave: 32.7\n",
      "Dave: 35.4\n",
      "Dave: 30.9\n",
      "Dave: 31.3\n",
      "Dave: 30.3\n",
      "Dave: 31.3\n",
      "Dave: 31.8\n",
      "Dave: 31.0\n",
      "Dave: 31.5\n",
      "Dave: 31.1\n",
      "Dave: 30.1\n",
      "Dave: 31.1\n",
      "Dave: 30.7\n",
      "Dave: 31.9\n",
      "Dave: 31.8\n",
      "Dave: 31.6\n",
      "Dave: 30.0\n",
      "Dave: 31.7\n",
      "Dave: 30.5\n",
      "Dave: 30.2\n",
      "Dave: 30.6\n",
      "Dave: 30.6\n"
     ]
    }
   ],
   "source": [
    "# Start WebCam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "count = -1\n",
    "status = 1\n",
    "\n",
    "while True:\n",
    "    # Get Frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Detect and predict mask using the trained MobileNet Mask Detector\n",
    "    (locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n",
    "    \n",
    "    # Iterate over all predictions found in the frame\n",
    "    for (box, pred) in zip(locs, preds):\n",
    "        \n",
    "        # Get bounding box\n",
    "        (startX, startY, endX, endY) = box\n",
    "        \n",
    "        # Get mask prediction outcome\n",
    "        (mask, withoutMask) = pred\n",
    "    \n",
    "        # Identify subject in the frame using the trained LBPH Face Recognizer\n",
    "        res_name, res_conf = detect_and_recognize(frame, lbph, box)\n",
    "        \n",
    "        # If confidence level is under 70 it passes the threshold and prediction is valid\n",
    "        if res_conf < 70:\n",
    "            \n",
    "            # Print prediction result and its corresponding confidence level\n",
    "            print(\"{}: {:}\".format(res_name, math.floor((100 - res_conf)*10)/10))\n",
    "            \n",
    "            # If probability of mask greater than without mask, then subject is wearing a mask\n",
    "            if mask > withoutMask:\n",
    "                label = \"Thanks for wearing a mask, {}!\".format(res_name)\n",
    "                color = (0,255,0)\n",
    "            # Otherwise, subject is not wearing a mask\n",
    "            else:\n",
    "                label = \"Please wear a mask, {}!\".format(res_name)\n",
    "                color = (0,0,255)\n",
    "            \n",
    "            # Write the text that includes the name of the subject, and\n",
    "            # whether or not subject is wearing a mask\n",
    "            cv2.putText(frame, label, (15,35),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "            \n",
    "            # Annotate the bounding box of the subject in the frame\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "    # Define quit key\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
